{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2 # Batch size, sequence length, and number of channels\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x11c954f20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# the mat mul operation\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "# rows times columns\n",
    "c = a @ b\n",
    "\n",
    "print (a)\n",
    "print (b)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = weights @ x # (T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timestamp\n",
    "\n",
    "https://youtu.be/kCc8FmEb1nY?si=pq2FmeBHG37mZymE&t=3282\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# version 3: uses softmax for normalization\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros(T, T)\n",
    "\n",
    "# with the next operation we set the upper triangular part to -inf\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))  # set upper triangular to -inf\n",
    "weights = F.softmax(weights, dim=-1)  # apply softmax to normalize\n",
    "\n",
    "#the -inf is to ensure tokens from the past dont comunicati with tokens from the future\n",
    "xbow3 = weights @ x  # (T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow2, xbow3)  # should be True, both methods yield the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch size, time steps, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,16) * (B,16,T) -> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # (T,T) # don't consider future tokens\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # (B,T,T) # set 0 to -inf for softmax\n",
    "wei = F.softmax(wei, dim=-1) # (B,T,T) # apply softmax to sum to 1\n",
    "\n",
    "v = value(x)    # (B,T,16)\n",
    "output = wei @ v # (B,T,T) @ (B,T,16) -> (B,T,16) # attention output for each token in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrej Karpathy explanation\n",
    "# for a token, my information is kept in vector x\n",
    "# query, here is what I'm interested in\n",
    "# key, here is what I have\n",
    "# If you find me interesting, here is what I will communicate.\n",
    "\n",
    "# by default, these nodes (tokens) have no way of knowing where the are in the sequence\n",
    "# we can add positional encodings to the token embeddings to give them a sense of order\n",
    "\n",
    "# for encoder attentionk, the masked_prefil is not needed, because all tokens can attend to each other\n",
    "\n",
    "\n",
    "# self attention from vs cross-attention\n",
    "# In this case, it is self attention, because the output of the attention mechanism moves the embedding vector of the token itself based on interaction with the other tokens in the sequence\n",
    "# k, q, v are all derived from the same source, the token embeddings of the sequence itself\n",
    "\n",
    "# in cross-attention, the k and v are derived from a different source than the q\n",
    "# for example, in a language model, the k and v are derived from the token embeddings of the sequence itself, but the q is derived from the token embeddings of the sequence itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0010) tensor(1.0562) tensor(17.5985)\n",
      "tensor(1.0999)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from attention is all you need paper (2017)\n",
    "\n",
    "# we only need to add the division by sqrt(head_size) when calculating the attention weights,\n",
    "\n",
    "# why do we do this:\n",
    "# to prevent the dot products from growing too large in magnitude\n",
    "# its called \"scaled dot-product attention\"\n",
    "\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # * head_size**-0.5 # (B,T,16) * (B,16,T) -> (B,T,T)\n",
    "\n",
    "print(k.var(), q.var(), wei.var())\n",
    "# variance of wei is in the order of head_size\n",
    "\n",
    "# but if we scale it down by sqrt(head_size)\n",
    "wei = wei * head_size**-0.5\n",
    "print(wei.var())\n",
    "\n",
    "# wei feeds into softmax, and softmax is sensitive to large input values\n",
    "# this means softmax results in one-hot like vectors, which is not what we want\n",
    "\n",
    "# normally distributed inputs to softmax\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
    "\n",
    "# sharpen towards the max\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)\n",
    "\n",
    "# so we scale it down by sqrt(head_size) to prevent the dot products from growing too large in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
